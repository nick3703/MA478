---
title: "Quiz7 - MA478"
author: "Clark"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Once again let's consider the `wbca` dataset. Create a model to predict malignency of tumor based on marginal adhesion. Compare a model with a probit link to a logit link. Which one is preferred?



```{r, warning=FALSE,message=FALSE}
library(faraway)
library(tidyverse)
data(wbca)

wbca_mod <- wbca %>%
  mutate(malig = ifelse(Class==0,1,0))

mod_prob <- glm(malig~Adhes,data=wbca_mod,family=binomial(link="probit"))
mod_log <- glm(malig~Adhes,data=wbca_mod,family=binomial(link="logit"))

AIC(mod_prob)
AIC(mod_log)

```


create an ROC curve for your preferred model. Determine an appropriate threshold where the sensitivity is over 60\% and the specificity is over 97\%.


```{r,warning=FALSE,message=FALSE}
library(pROC)
ROC_Curve <- roc(as.factor(wbca_mod$malig),predict(mod_log, type="response"))

plot(ROC_Curve)

ROC_Curve$sensitivities
ROC_Curve$specificities
ROC_Curve$thresholds
```



Explain why the specificities are so much higher than the sensitivities for this dataset.


There's only a single predictor so the model likely is not sufficient, which we can see:

```{r,warning=FALSE,message=FALSE}
library(glmtoolbox)
hltest(mod_log)


```

Because of this, the model is likely defaulting to predicting most of our data as the smallest class (0s)

```{r}

predict(mod_log, type="response")%>%median()
```

Inflating our specificity for most threshold values. BL is, this isn't a very good model.