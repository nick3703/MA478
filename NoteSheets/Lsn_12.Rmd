---
title: "Lesson 12"
author: "Clark"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Let's consider data on the career choice of some high school students

```{r,warning=FALSE,message=FALSE}
library(faraway)
library(tidyverse)
library(nnet)

data(hsb)

```

The purpose of this study was to determine which factors are related to the choice of the type of program: academic, vocational, or general, that students pursue in high school

Should this data be considered categorical or ordinal?

\vspace{.5in}

We can explore the data in a few different ways:

```{r}
#qplot(hsb$prog, geom = "bar")
#pairs(hsb)
#table(hsb$gender, hsb$prog)
#table(hsb$ses, hsb$prog)
#pairs(prog ~ gender + write + math, data = hsb)

```

However, most of these are less than satisfying. We can build out a baseline-categories multinomial model. Perhaps we want `general` to be the baseline

```{r}
hsb$prog <- relevel(hsb$prog, ref = "general")
mmod <- multinom(prog ~ gender+race+ses+schtyp+read+
                   write, hsb)
confint(mmod)
```

What do we see here?

\vspace{1.in}

P values can be computed manually

```{r}

z_stats <- summary(mmod)$coefficients / summary(mmod)$standard.errors
p_vals <- (1-pnorm(abs(z_stats), 0, 1))*2
p_vals

```

Perhaps we want to compare to a simpler model that does not include `race` and `gender`

```{r}
smaller_mod <- multinom(prog ~ ses+schtyp+read+
                   write, hsb)

anova(smaller_mod,mmod,test="Chisq")

```

What does this suggest?

\vspace{1.in}

Let's dig into this a bit more. If we just look at socio-economic status we can turn our data into grouped data.

```{r}
hsb_ses <- hsb %>%
  group_by(ses, prog) %>%
  summarise(count=n()) %>%
  mutate(ses_tot = sum(count), 
         prog_prop = count/ses_tot)


ggplot(hsb_ses, aes(x = ses, y = prog_prop,
                     group = prog, color = prog))+
  geom_line() +
  ggtitle("Proportion of Program Selection by SES")
```

With the data structured like this, if our primary aim is to analyze the impact of ses on program, we can also view the data as a contingency table

```{r}
ct <- xtabs(count~ses+prog,data=hsb_ses)
ct

```

Nowm one of the benefits of using a contingency table is it allows us to calculate one of the more famous statistical tests. Pearson's $\chi^2$ test. The construct of this test, which you may have seen before, is relatively straight forward.


\vspace{1.5in}

```{r}
summary(ct)
```

The degrees of freedom are the number of rows minus one times the number of columns minus one.

This analysis assumes that we set out to sample 200 students (which is probably a good assumption in this case). However, it's also possible that we just recorded every student who stopped into our office over the course of a month. In this case, we can't think about our data coming from a multinomial distribution (with a fixed number of trials). Rather we would assume that


\vspace{1.in}

From here we can build out a Poisson regression model (more to follow next lesson)


\vspace{1.in}

```{r}
test <- glm(count~ses+prog,data=hsb_ses,family=poisson)
summary(test)

```

However, really all this tells us is that we have more middle income than high income and more `general` and `vocation` than academic. It does not tell us the relationship between ses and program. 

To do this, we can conduct correspondence analysis (CA). This is going to feel a lot like PCA, but it is for count data whereas PCA is for continuous data. Without getting too much into the weeds on this, correspondence analysis uses a $\chi^2$ distance between each of the observations. PCA works by looking at the Mahalonobis distance (sort of, I don't want to get too much into the weeds here). But, if you recall to compute PCA you compute the SVD of the X matrix. For CA you compute the SVD of the Pearson residuals from the Poisson regression model


```{r}



z<-xtabs(residuals(test,type="pearson")~ses+prog,hsb_ses)

blah <-svd(z,3,3)

leftsv <- blah$u %*% diag(sqrt(blah$d[1:3]))

rightsv <- blah$v %*% diag(sqrt(blah$d[1:3]))

l1 <- 1.1*max(abs(rightsv),abs(leftsv))

plot(rbind(leftsv,rightsv),asp=1,xlim=c(-l1,l1),
     ylim=c(-l1,l1),xlab="SV1",ylab="SV2",type="n")
abline(h=0,v=0)
text(leftsv,dimnames(z)[[1]])
text(rightsv,dimnames(z)[[2]])

```

Another way the relationships (as a form of descriptive analytics) to view this is through a mosaic plot

```{r}
mosaicplot(ct,color=TRUE,main=NULL,las = 1)
```

Here we can see what the CA is giving us. Apperantly there is an R package `FactoMineR` that has a function called `CA` that does the correspondence analysis. It's kind of nice

```{r}
library(FactoMineR)
CA(ct)

```


